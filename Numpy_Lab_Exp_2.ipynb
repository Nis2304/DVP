{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a0ab55b7",
      "metadata": {
        "id": "a0ab55b7"
      },
      "source": [
        "# Ex No 2.a Analyzing Academic Performance\n",
        "Problem Statement:\n",
        "Assess the performance trends of students across different subjects, focusing on data imperfections such as missing values, duplicates, and the need to uniquely identify records.\n",
        "\n",
        "Objective Scenario:\n",
        "A high school intends to analyze semester results to enhance teaching strategies and provide targeted support. The dataset includes student roll numbers but contains issues like missing entries and duplicates that must be addressed for accurate analysis.\n",
        "\n",
        "Dataset:\n",
        "Data is provided as a Python list containing tuples for each student's roll number followed by their scores in Mathematics, Science, and English. The list includes missing entries (represented as None) and intentionally duplicated records. Example: [(101, 45, 78, None), (102, 65, 56, 77), (103, 95, 85, 92), (102, 65, 56, 77), (104, 45, None, 88), (101, 45, 78, None)].\n",
        "\n",
        "Tasks to be performed:\n",
        "\n",
        "Data Conversion and Inspection:\n",
        "\n",
        "Convert the list of student records into a Numpy array.\n",
        "Identify and count the number of missing values in each subject.\n",
        "Detect duplicate entries based on student roll numbers.\n",
        "\n",
        "Data Cleaning:\n",
        "Handle missing values by replacing them with the median score of the respective subject.\n",
        "Remove duplicate records, ensuring data integrity by retaining only the first occurrence of each student's record.\n",
        "\n",
        "Data Normalization:\n",
        "Apply min-max normalization to the scores (excluding roll numbers) to scale them between 0 and 1. This adjustment facilitates fair comparisons across different subjects.\n",
        "\n",
        "Statistical Analysis:\n",
        "Calculate the normalized mean, median, and standard deviation of scores for each subject.\n",
        "Identify the subject with the highest variability in scores."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Dataset Setup\n",
        "# -------------------------------\n",
        "data = [\n",
        "    (101, 45, 78, None),\n",
        "    (102, 65, 56, 77),\n",
        "    (103, 95, 85, 92),\n",
        "    (102, 65, 56, 77),\n",
        "    (104, 45, None, 88),\n",
        "    (101, 45, 78, None)\n",
        "]\n",
        "\n",
        "# Convert to NumPy array (object type to allow None)\n",
        "arr = np.array(data, dtype=object)\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Data Inspection\n",
        "# -------------------------------\n",
        "# Count missing values per subject (columns 1:3)\n",
        "missing_counts = [(arr[:, i] == None).sum() for i in range(1, 4)]\n",
        "print(\"Missing values per subject (Math, Science, English):\", missing_counts)\n",
        "\n",
        "# Detect duplicate roll numbers\n",
        "_, unique_indices = np.unique(arr[:, 0], return_index=True)\n",
        "duplicate_indices = np.setdiff1d(np.arange(arr.shape[0]), unique_indices)\n",
        "print(\"Duplicate entries based on roll number:\", arr[duplicate_indices])\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Data Cleaning\n",
        "# -------------------------------\n",
        "# Replace None with median per subject\n",
        "cleaned_arr = arr.copy()\n",
        "for col in range(1, 4):\n",
        "    # Get column without None values\n",
        "    col_values = [x for x in cleaned_arr[:, col] if x is not None]\n",
        "    median_val = np.median(col_values)\n",
        "    # Replace None with median\n",
        "    cleaned_arr[:, col] = [median_val if x is None else x for x in cleaned_arr[:, col]]\n",
        "\n",
        "# Remove duplicates, keeping first occurrence\n",
        "cleaned_arr = cleaned_arr[np.sort(unique_indices)]\n",
        "\n",
        "print(\"\\nCleaned Data:\\n\", cleaned_arr)\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Data Normalization (Min-Max)\n",
        "# -------------------------------\n",
        "scores = cleaned_arr[:, 1:].astype(float)\n",
        "min_vals = scores.min(axis=0)\n",
        "max_vals = scores.max(axis=0)\n",
        "normalized_scores = (scores - min_vals) / (max_vals - min_vals)\n",
        "\n",
        "print(\"\\nNormalized Scores:\\n\", normalized_scores)\n",
        "\n",
        "# -------------------------------\n",
        "# 5. Statistical Analysis\n",
        "# -------------------------------\n",
        "means = normalized_scores.mean(axis=0)\n",
        "medians = np.median(normalized_scores, axis=0)\n",
        "std_devs = normalized_scores.std(axis=0)\n",
        "\n",
        "subjects = [\"Mathematics\", \"Science\", \"English\"]\n",
        "variability_subject = subjects[np.argmax(std_devs)]\n",
        "\n",
        "print(\"\\nNormalized Means:\", means)\n",
        "print(\"Normalized Medians:\", medians)\n",
        "print(\"Normalized Standard Deviations:\", std_devs)\n",
        "print(\"Subject with highest variability:\", variability_subject)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vplu09QhnWB5",
        "outputId": "201767a2-2855-4c5c-d1a2-3993ff4d9fb8"
      },
      "id": "vplu09QhnWB5",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values per subject (Math, Science, English): [np.int64(0), np.int64(1), np.int64(2)]\n",
            "Duplicate entries based on roll number: [[102 65 56 77]\n",
            " [101 45 78 None]]\n",
            "\n",
            "Cleaned Data:\n",
            " [[101 45 78 np.float64(82.5)]\n",
            " [102 65 56 77]\n",
            " [103 95 85 92]\n",
            " [104 45 np.float64(78.0) 88]]\n",
            "\n",
            "Normalized Scores:\n",
            " [[0.         0.75862069 0.36666667]\n",
            " [0.4        0.         0.        ]\n",
            " [1.         1.         1.        ]\n",
            " [0.         0.75862069 0.73333333]]\n",
            "\n",
            "Normalized Means: [0.35       0.62931034 0.525     ]\n",
            "Normalized Medians: [0.2        0.75862069 0.55      ]\n",
            "Normalized Standard Deviations: [0.40926764 0.37645872 0.37739973]\n",
            "Subject with highest variability: Mathematics\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62e96f76",
      "metadata": {
        "id": "62e96f76"
      },
      "source": [
        "# Ex No 2.b Analyzing Healthcare Service Efficiency\n",
        "Problem Statement:\n",
        "Evaluate the performance trends of different hospital departments, addressing issues such as missing data, duplicates, and the need for unique identifiers.\n",
        "\n",
        "Objective Scenario:\n",
        "A hospital's administration wants to analyze patient treatment outcomes to enhance service delivery and provide targeted improvements in care. The dataset includes department ID numbers but contains issues like missing entries and duplicates that need accurate resolution for effective analysis.\n",
        "\n",
        "Dataset:\n",
        "Data is provided as a Python list containing tuples for each department's ID followed by their performance scores in patient satisfaction, treatment success rate, and wait times. The list includes missing entries (represented as None) and intentionally duplicated records. Example: [(701, 90, 95, None), (702, 88, 90, 85), (703, 92, None, 80), (702, 88, 90, 85), (704, 85, None, 78), (701, 90, 95, None)].\n",
        "\n",
        "Tasks to be performed:\n",
        "\n",
        "Data Conversion and Inspection:\n",
        "\n",
        "Convert the list of department records into a Numpy array.\n",
        "Identify and count the number of missing values in each performance metric.\n",
        "Detect duplicate entries based on department ID numbers.\n",
        "\n",
        "Data Cleaning:\n",
        "\n",
        "Handle missing values by replacing them with the median score of the respective metric.\n",
        "Remove duplicate records, ensuring data integrity by retaining only the first occurrence of each department's record.\n",
        "\n",
        "Data Normalization:\n",
        "\n",
        "Apply min-max normalization to the scores (excluding ID numbers) to scale them between 0 and 1. This adjustment facilitates fair comparisons across different metrics.\n",
        "\n",
        "Statistical Analysis:\n",
        "\n",
        "Calculate the normalized mean, median, and standard deviation of scores for each performance metric.\n",
        "Identify the metric with the highest variability in scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6f19b63a",
      "metadata": {
        "id": "6f19b63a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "486b14f8-23f7-4e6a-d102-e475ae1c9c16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values per metric (Satisfaction, Success, Wait time): [np.int64(0), np.int64(2), np.int64(2)]\n",
            "Duplicate entries based on Dept ID:\n",
            " [[702 88 90 85]\n",
            " [701 90 95 None]]\n",
            "\n",
            "Cleaned Data:\n",
            " [[701 90 95 np.float64(82.5)]\n",
            " [702 88 90 85]\n",
            " [703 92 np.float64(92.5) 80]\n",
            " [704 85 np.float64(92.5) 78]]\n",
            "\n",
            "Normalized Scores:\n",
            " [[0.71428571 1.         0.64285714]\n",
            " [0.42857143 0.         1.        ]\n",
            " [1.         0.5        0.28571429]\n",
            " [0.         0.5        0.        ]]\n",
            "\n",
            "Normalized Means: [0.53571429 0.5        0.48214286]\n",
            "Normalized Medians: [0.57142857 0.5        0.46428571]\n",
            "Normalized Std Deviations: [0.36943144 0.35355339 0.37584938]\n",
            "Metric with highest variability: Wait Time\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# --------------------------------------\n",
        "# 1. Dataset Setup\n",
        "# --------------------------------------\n",
        "data = [\n",
        "    (701, 90, 95, None),\n",
        "    (702, 88, 90, 85),\n",
        "    (703, 92, None, 80),\n",
        "    (702, 88, 90, 85),\n",
        "    (704, 85, None, 78),\n",
        "    (701, 90, 95, None)\n",
        "]\n",
        "\n",
        "# Convert to NumPy array (object type to allow None values)\n",
        "arr = np.array(data, dtype=object)\n",
        "\n",
        "# --------------------------------------\n",
        "# 2. Data Inspection\n",
        "# --------------------------------------\n",
        "# Count missing values in each metric\n",
        "missing_counts = [(arr[:, i] == None).sum() for i in range(1, 4)]\n",
        "print(\"Missing values per metric (Satisfaction, Success, Wait time):\", missing_counts)\n",
        "\n",
        "# Detect duplicate department IDs\n",
        "_, unique_indices = np.unique(arr[:, 0], return_index=True)\n",
        "duplicate_indices = np.setdiff1d(np.arange(arr.shape[0]), unique_indices)\n",
        "print(\"Duplicate entries based on Dept ID:\\n\", arr[duplicate_indices])\n",
        "\n",
        "# --------------------------------------\n",
        "# 3. Data Cleaning\n",
        "# --------------------------------------\n",
        "cleaned_arr = arr.copy()\n",
        "\n",
        "# Replace None with median of the column\n",
        "for col in range(1, 4):\n",
        "    col_values = [x for x in cleaned_arr[:, col] if x is not None]\n",
        "    median_val = np.median(col_values)\n",
        "    cleaned_arr[:, col] = [median_val if x is None else x for x in cleaned_arr[:, col]]\n",
        "\n",
        "# Remove duplicates, keeping first occurrence\n",
        "cleaned_arr = cleaned_arr[np.sort(unique_indices)]\n",
        "\n",
        "print(\"\\nCleaned Data:\\n\", cleaned_arr)\n",
        "\n",
        "# --------------------------------------\n",
        "# 4. Data Normalization (Min-Max)\n",
        "# --------------------------------------\n",
        "scores = cleaned_arr[:, 1:].astype(float)\n",
        "min_vals = scores.min(axis=0)\n",
        "max_vals = scores.max(axis=0)\n",
        "normalized_scores = (scores - min_vals) / (max_vals - min_vals)\n",
        "\n",
        "print(\"\\nNormalized Scores:\\n\", normalized_scores)\n",
        "\n",
        "# --------------------------------------\n",
        "# 5. Statistical Analysis\n",
        "# --------------------------------------\n",
        "means = normalized_scores.mean(axis=0)\n",
        "medians = np.median(normalized_scores, axis=0)\n",
        "std_devs = normalized_scores.std(axis=0)\n",
        "\n",
        "metrics = [\"Patient Satisfaction\", \"Treatment Success\", \"Wait Time\"]\n",
        "highest_var_metric = metrics[np.argmax(std_devs)]\n",
        "\n",
        "print(\"\\nNormalized Means:\", means)\n",
        "print(\"Normalized Medians:\", medians)\n",
        "print(\"Normalized Std Deviations:\", std_devs)\n",
        "print(\"Metric with highest variability:\", highest_var_metric)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}